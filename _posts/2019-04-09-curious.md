---
layout:     post
title:      Intrinsically Motivated Multi-Task Multi-Goal RL
date:       2019-04-09 11:21:29
summary:    Curious can target multiple multi-goal task using a single policy. It is intrinsically motivated to choose its own tasks and goals. It tracks its own competence and competence progress and focuses on tasks with high progress. This enables efficient learning, resistance to distracting tasks, forgetting and sensory failures. 
categories: jekyll
permalink: curious_intrinsically_motivated_multi_task_multi_goal_rl
use_math: true
---



<div align="center">
<iframe width="70%" height="300" src="https://www.youtube.com/embed/F8KQu8eNhcg" frameborder="0" allowfullscreen></iframe>
</div>



## Introduction

In open-ended and changing environments, agents face a wide range of potential tasks that might not
come with associated reward functions. Such autonomous learning agents must set their own tasks
and build their own curriculum through an intrinsically motivated exploration. Because some tasks
might prove easy and some impossible, agents must actively select which task to practice at any
given moment, to maximize their overall mastery on the set of learnable tasks. 

This blog post presents CURIOUS, an algorithm that leverages: 1) an extension of Universal Value Function Approximators to achieve, within a unique policy, multiple tasks, each parameterized by multiple goals and 2) an automated curriculum learning mechanism that biases the attention of the agent towards tasks maximizing the absolute learning progress. Agents focus on achievable tasks first, and focus back on tasks that are being forgotten.


Experiments conducted in a new multi-task multi-goal robotic environment show that our algorithm benefits from these two ideas and demonstrate properties of robustness to distracting tasks, forgetting and changes in body properties. All details can be found in the [paper](https://arxiv.org/abs/1810.06284). The algorithm and the environment can be found on Github. 



## The Problem of Intrinsically Motivated Multi-Task, Multi-Goal Reinforcement Learning

<div align="center">
<img class="80" src="https://openlab-flowers.inria.fr/uploads/default/original/2X/1/15ea4a22bd3ebbe32ad0b9afddd36b9647563c34.png" width="80%" alt="The Multi-Task, Multi-Goal Fetch Arm environment." />
<div>
<sub>
<i><b>Multi-Task, Multi-Goal Fetch Arm</b>: an environment with multiple potential tasks with various levels of difficulty, from simple to impossible (Reach, Push, Pick and Place, Stack, Push out-of-reach cube). Each task is parameterized by a goal (target).</i></sub>
</div>
</div>

Agents in the real world might face a large number of potential _tasks_. A domestic robot might want to clean up a table, to prepare the meal, to set the table etc. In addition, some tasks might be parameterized by _goals_ or _targets_. If "move the plates" can be seen as a task, the _goal_ indicates where they should be moved (e.g. to the closet). The goal is here defined as a way to parameterize the task. This setting is represented in our Multi-Task Multi-Goal Fetch Arm environment. Adapted from [OpenAI Gym](https://github.com/openai/gym)'s Fetch Arm environments, the robotic arm faces a table and several cubes, and can decide to _Reach_ a 3D target (goal) with its gripper, to _Push_ a cube on a 2D target, to _Pick and Place_ a cube on a 3D target or to _Stack_ one cube on top of another. Several out-of-reach cubes are added to the scene, to represent _distacting tasks_, tasks that are impossible to solve by the agent.

This problem is seen through the lens of the [Intrinsicually Motivated Goal Exploration Process](https://arxiv.org/abs/1708.02190) (IMGEP)  framework. The agent decides itself which task and goal to target, which task and goal to train on at any given moment. It is intrinsically motivated to set its own goals to explore its surroundings, with the objective of mastering all tasks that could be mastered. The number of tasks might be large, some tasks might be easy, others difficult or even impossible. This advocates for curriculum learning mechanism to enable intelligent experience collection and training.

## Previous Work

CURIOUS builds on previous research in Multi-Goal RL and Muti-Task RL to propose a new policy encoding to target multiple tasks parameterized by multiple goals. [Universal Value Function Approximators](http://proceedings.mlr.press/v37/schaul15.pdf) (UVFA) proposed to condition the policy and the value function by the current goal in a multi-goal setting. This enables to target goals drawn from a continuous space (e.g. target maze location, target gripper position) and efficient generalization across goals.  [Hindsight Experience Replay](https://arxiv.org/abs/1707.01495) (HER) proposed to generate imagined goals to learn about, when a trajectory did not achieve its original goal (counterfactual learning). [UNICORN](https://arxiv.org/abs/1802.08294) introduced a task-conditioned policy to target a set of discrete tasks and used discrete counterfactual learning (replacing the original task by a random imagined task from the task-set.

CURIOUS also builds on papers from the developmental robotics community to implement a mechanism for autonomous curriculum generation. As in [MACOB](https://hal.archives-ouvertes.fr/hal-01384566/document) and the [IMGEP](https://arxiv.org/abs/1708.02190) framework, CURIOUS tracks its competence and learning progress on each task and maximizes learning progress based on a multi-armed bandit algorithm.

## A Modular Goal Encoding: E-UVFA

The most intuitive way to target multiple multi-goal tasks would be to use a multi-goal policy for each task. We call this architecture _Multi-Goal Task Expert_ (MG-TE). 

With CURIOUS, we propose the _Extended-UVFA_ encoding to target multiple tasks potentially parameterized by multiple goals. The input to the policy (and value function) is now the concatenation of the current state, a one-hot encoding of the task and a goal vector. The goal vector is the concatenation of the goals in each task, where the goals of unconsidered tasks are set to $0$. In the figure, the agent targets task $T_1$ $(t_d=[1, 0])$ out of $2$ tasks and targets the 2D goal $g_1 = [g_{11},g_{12}]$ for task $T_1$, e.g. Pushing the yellow cube at position $g_1$ on the table. The underlying learning is [Deep Deterministic Policy Gradient](https://arxiv.org/abs/1509.02971) (DDPG). We use discrete counterfactual learning for cross-task learning and HER for counterfactual goal learning. This consists in replacing the original task descriptor and goal in the transition by others. HER replaces the original goal by an outcome achieved later in the trajectory. UNICORN replaces the original task by a random task from the task-set.




<div align="center">
<img src="https://openlab-flowers.inria.fr/uploads/default/original/2X/5/5d3296b49ab9f1909dcefe97795c7af5debc0460.png" width="70%" alt="The E-UVFA architecture" />
<div>
<sub>
<i><b>Actor-Critic networks using the E-UVFA architecture</b>: In green a discrete one-hot encoding of the current task. In yellow the goal vector, concatenation of the goal vectors (targets) of each task. When a task is selected, only the sub-vector corresponding to that task is activated. </i></sub>
</div>
</div>


The figure below demonstrates the advantage of using a unique policy and value function to target all tasks and goals at once. We run $10$ trials for each architecture on a set of $4$ tasks and report the average success rate over the four tasks. As a sanity check demonstrating the need to use a modular representation of tasks, we try the HER algorithm, where goals are drawn from a flat representation (e.g. put the cube at position $x_1$, while reaching position $x_2$ with the gripper). As almost none of these goals can be reached in practice, the performance of HER stays null.


<div align="center">
<img src="https://openlab-flowers.inria.fr/uploads/default/original/2X/6/668443d3bbea2c4a5627fba9e3e5b6e2db4dbabd.png"  width="80%" alt="The E-UVFA architecture" />
<div>
<sub>
<i><b>Impact of the policy and value function architecture.</b> Average success rates computed over achievable tasks. Mean +/- standard deviation over 10 trials are plotted, while dots indicate significance when testing E-UVFA against MG-TE with a Welch's t-test. </i></sub>
</div>
</div>



## Automatic Curriculum with Learning Progress

Our agent tracks its competence and learning progress (LP) on each task. To do that, it performs self-evaluation episodes without exploration noise, and records for each task the list of past successes and failures. The competence in a task is simply the success rate over the recent history. The learning progress is defined as the derivative of the competence, and is empirically computed using a difference of success rates computed over two consecutive and non-overlapping windows from the recent history.

The learning progress measures are used for two purposes:
* To select which task to target next (as in MACOB).
* To select which task to train on (new).

The problem of tasks selection can be seen as a non-stationary multi-armed bandit problem, where the value to maximize is learning progress. We compute selection probabilities using an epsilon-greedy proportion rule based on the absolute measures of learning progress:

$$
p(T_i) = \frac{\epsilon}{N} + (1-\epsilon) \frac{\mid LP(T_i)\mid}{\sum_j \mid LP(T_j)\mid},
$$

where $N$ is the number of tasks, $LP(T_i)$ is the learning progress computed on task $T_i$. These probabilities are used to select the next task to target, and to bias the counterfactual learning of tasks. Substituting the original task by another enables to focus learning on the substitute task. Using LP measures therefore enables the agent to control on which task to focus its learning, which builds an automatic curriculum strategy.

The figure below shows the competence, learning progress and selection probabilities computed internally by the agent. It is like having access to the inner variables it uses to make decisions. We interpret these curves as a developmental trajectory of the agent. First, it learns how to control its gripper ($T_1$, blue). When it knows how to, learning progress drops, making this task less interesting. It then focuses on another task where it has started to make progress (pushing the cube, orange). Finally, it learns to pick and place and stack cubes (green and yellow respectively). Around $75.10^3$ episodes, the agent detects a drop in its competence in the Pick and Place task, this triggers an increase of the absolute progress which ultimately results in a renewed focus on that task, enabling to mitigate the performance drop.


<div align="center">
<table>
<tr>
<td>
<img class='special' src="https://openlab-flowers.inria.fr/uploads/default/original/2X/0/0e627db6b4126c779880462d21ef39ecdf0f35a2.png" height="150"   />
</td>
<td>
<img class='special' src="https://openlab-flowers.inria.fr/uploads/default/original/2X/e/e1ef389a8fa7aa109e9a2561f0aec81881fcff95.png" height="150"   />
</td>
<td>
<img class='special' src="https://openlab-flowers.inria.fr/uploads/default/original/2X/b/b799cd450ecfd4fcea465b76ccc6ca28ac498632.png" height="150"  />
</td>
</tr>
</table>
</div>
<sub>
<i><b>Competence, learning progress and developmental trajectories</b>: Left: competence for each task in one run of the algorithm. Middle:  corresponding absolute learning progress. Right: corresponding task probabilities.</i></sub>




## Resilience to Distracting Tasks

In the real world, not all tasks can be achieved. We simulate this with extra tasks where the agent needs to push out-of-reach cubes on 2D locations. As these tasks are impossible, the learning progress measure stays flat, which enables the agent to focus on more relevant tasks. When the number of distracting tasks increases $(0,4,7)$ in addition to the set of four tasks described earlier, the use of the learning progress task selection and replay (CURIOUS) improves over the random task selection and replay (E-UVFA only).


<div align="center">
<img src="https://openlab-flowers.inria.fr/uploads/default/original/2X/7/73e801d28a024ea602c765a97abea092e5e3e6df.png" width="80%" alt="The E-UVFA architecture" />
<div>
<sub>
<i><b>Resilience to distracting tasks</b>: Different colors represent different number of distracting tasks (Pushing an out-o-reach cube). There are four achievable tasks. Dots indicate significant differences between CURIOUS (intrinsically motivated) and E-UVFA (random task), using a Welch's t-test and 10 seeds. Mean and standard error of the mean plotted. </i></sub>
</div>
</div>



## Resilience to Forgetting and Sensory Failures


Using absolute learning progress measures enables the agent to detect drops in performance. Here, we simulate a time-locked sensory failure: the sensor reporting the position of one of the cube is shifted by the size of a cube. The performance on the Push task related to that cube (one of the four tasks) suddenly drops, making the average success rate over all tasks drop by a quarter (see figure below). We then compare E-UVFA (random task selection and replay) and CURIOUS (using LP) during the recovery. CURIOUS manages to recover $95\%$ of its pre-perturbation performance $45\%$ faster than its random counterpart.


<div align="center">
<img src="https://openlab-flowers.inria.fr/uploads/default/original/2X/3/3af006ba74e359c9970c99cc6c339e08872faad7.png" width="80%" alt="The E-UVFA architecture" />
<div>
<sub>
<i><b>Resilience to sensory failure</b>: Recovery following a sensory failure. CURIOUS recovers 90% of its original performance twice as fast as E-UVFA. Dots indicate significant differences in mean performance (Welch's t-test, 10 random seeds). Mean and standard deviations are reported.</i></sub>
</div>
</div>


## Discussion

As noted in [Mankowitz et al., 2018](https://arxiv.org/abs/1802.08294), representations of the world state are learned in the first layers of a neural network policy/value function. Sharing these representations across all tasks and goals explains the important difference between the E-UVFA encoding and the use of multiple task-expert policies. However, learning all tasks in the same policy might become difficult as the number of tasks increases, and when tasks are different from one another (e.g. using different sensory modalities). Catastrophic forgetting can also play a role, as previously mastered tasks might be forgotten because the agent targets them less often. Although this last point is partially mitigated by the use of learning progress for task replay, it might be a good idea to consider several multi-task, multi-goal policies when the number of task increases.

CURIOUS is an algorithm able to tackle the problem of intrinsically motivated multi-task multi-goal reinforcement learning. This problem has rarely been considered in the past, only [MACOB](https://hal.archives-ouvertes.fr/hal-01384566/document) targeted that problem and proposed a solution based on population-based and memory-based algorithms. It is a problem of importance for autonomous lifelong learning, where agents must learn and act in a realistic world with multiple tasks of different difficulties, without having access to the reward functions.

In the future, CURIOUS could be used in a hierarchical manner. A higher-level policy could feed the sequence of tasks and goals for the lower level policy to target. This would replace the current one-step policy implemented by a multi-armed bandit algorithm. 

CURIOUS is given prior information about the set of potential tasks, their associated goal space and the reward function parameterized by tasks and goals. Further work should aim at reducing the importance of these priors. Several works go in that direction and propose autonomous learning of goal representation ([Laversanne-Finot et al., 2018](https://arxiv.org/abs/1807.01521), [Nair et al., 2018](https://arxiv.org/abs/1807.04742)). Goal selection policies could also be learned online using algorithms such as [SAGG-RIAC](https://arxiv.org/abs/1301.4862) or [GoalGAN](https://arxiv.org/abs/1705.06366).

## Conclusion
This blog post presents CURIOUS, a learning algorithm that combines an extension of UVFA to enable multi-task and multi-goal RL in a single policy (E-UVFA), and active mechanisms that bias the agent’s attention towards tasks where the absolute LP is maximized. With this mechanism, agents spend less time on impossible tasks and focus on achievable ones. It also helps to deal with forgetting, by refocusing learning on tasks that are being forgotten because of model faults, changes in the environment or body
changes (e.g. sensory failures). This mechanism is important for autonomous continual learning in the real world, where agents must set their own tasks and might face tasks with diverse levels of difficulty, some of which might be required to solve others later on.

## Links
* [Paper](https://arxiv.org/abs/1810.06284)
* [Code]()

## References
* [Intrinsicually Motivated Goal Exploration Process](https://arxiv.org/abs/1708.02190). Forestier et al., 2017.
* [Universal Value Function Approximators](http://proceedings.mlr.press/v37/schaul15.pdf). Schaul et al., 2015.
* [Hindsight Experience Replay](https://arxiv.org/abs/1707.01495). Andrychowicz et al., 2017.
* [Unicorn: Continual Learning with a Universal, Off-policy Agent](https://arxiv.org/abs/1802.08294). Mankowitz et al., 2018.
*  [Modular Active Curiosity-Driven Discovery of Tool Use](https://hal.archives-ouvertes.fr/hal-01384566/document). Forestier et al., 2016.
* [Continuous Control with Deep Reinforcement Learning](https://arxiv.org/abs/1509.02971). Lillicrap et al., 2015.
* [Curiosity Driven Exploration of Learned Disentangled Goal Spaces](https://arxiv.org/abs/1807.01521). Laversanne-Finot et al., 2018.
* [Visual Reinforcement Learning with Imagined Goals](https://arxiv.org/abs/1807.04742). Nair et al., 2018.
* [Automatic Goal Generation for Reinforcement Learning Agents](https://arxiv.org/abs/1705.06366). Florensa et al., 2017.
* [Active Learning of Inverse Models with Intrinsically Motivated Goal Exploration in Robots](https://arxiv.org/abs/1301.4862). Baranes and Oudeyer, 2013.



## Contact
Email: cedric.colas@inria.fr

-----------------
###### Subscribe to our [Twitter](https://twitter.com/@flowersINRIA).
###### Subscribe to our [RSS Feed](https://openlab-flowers.inria.fr/c/blog.rss).
###### Subscribe to our [mailing list](https://sympa.inria.fr/sympa/subscribe/flowers-blog).
